{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Image, Markdown\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca96b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardset vars\n",
    "dataset_path = \"data/processed/labeled_asset_dataset_enriched.csv\"\n",
    "config_path = \"config/generation_params.json\"\n",
    "alt_config = \"config/alt_scenario_generation_params.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04486bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for running the MLflow pipeline, resetting it, and displaying results\n",
    "def reset_pipeline():\n",
    "    \"\"\"Reset the pipeline by running the reset_pipeline.py script.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"scripts/reset_pipeline.py\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        encoding=\"utf-8\",\n",
    "        errors=\"replace\"  # prevent crashes from non-UTF-8 characters\n",
    "    )\n",
    "\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(result.stderr)\n",
    "\n",
    "\n",
    "def display_config(config_path):\n",
    "    \"\"\"Load and display the contents of a configuration file as formatted JSON.\"\"\"\n",
    "    if not os.path.exists(config_path):\n",
    "        print(f\"⚠️ Config file not found: {config_path}\")\n",
    "        return\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    display(Markdown(f\"### Contents of `{config_path}`:\"))\n",
    "    display(Markdown(f\"```json\\n{json.dumps(params, indent=4)}\\n```\"))\n",
    "\n",
    "\n",
    "def run_pipeline(dataset_path, alt_config=None, steps=None):\n",
    "    \"\"\"\n",
    "    Run the MLflow pipeline fully or step-by-step.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset CSV.\n",
    "        alt_config (str, optional): Alternate config file for the generate step.\n",
    "        steps (list, optional): Specific pipeline steps to run. \n",
    "                                If None, runs the full pipeline entry point.\n",
    "    \"\"\"\n",
    "    # If dataset exists and no steps were specified, just load the data\n",
    "    if steps is None:\n",
    "        try:\n",
    "            df = pd.read_csv(dataset_path)\n",
    "            print(\"✅ Dataset loaded.\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️ {dataset_path} not found. Running full MLflow pipeline...\")\n",
    "            result = subprocess.run(\n",
    "                [\"mlflow\", \"run\", \".\", \"-e\", \"pipeline\", \"--env-manager=local\"],\n",
    "                capture_output=True, text=True, encoding=\"utf-8\", errors=\"replace\"\n",
    "            )\n",
    "            print(result.stdout)\n",
    "            if result.returncode != 0:\n",
    "                print(\"❌ Full MLflow pipeline failed.\")\n",
    "                raise RuntimeError(\"Pipeline execution failed.\")\n",
    "            if os.path.exists(dataset_path):\n",
    "                print(\"✅ Dataset generated and loaded.\")\n",
    "                return pd.read_csv(dataset_path)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Dataset still not found at {dataset_path} after pipeline run.\")\n",
    "    else:\n",
    "        # Step-by-step pipeline execution\n",
    "        step_cmds = []\n",
    "        for step in steps:\n",
    "            if step == \"generate\":\n",
    "                cmd = [\n",
    "                    \"mlflow\", \"run\", \".\", \"-e\", \"generate\", \"--env-manager=local\"\n",
    "                ]\n",
    "                if alt_config:\n",
    "                    cmd.extend([\"-P\", f\"config={alt_config}\"])\n",
    "                step_cmds.append((\"Generate\", cmd))\n",
    "            elif step == \"prepare\":\n",
    "                step_cmds.append((\"Prepare\", [\"mlflow\", \"run\", \".\", \"-e\", \"prepare\", \"--env-manager=local\"]))\n",
    "            elif step == \"train-both\":\n",
    "                step_cmds.append((\"Train Both\", [\"mlflow\", \"run\", \".\", \"-e\", \"train-both\", \"--env-manager=local\"]))\n",
    "            else:\n",
    "                print(f\"⚠️ Unknown step: {step}\")\n",
    "\n",
    "        for name, cmd in step_cmds:\n",
    "            print(f\"\\n--- Running: {name} ---\\n{' '.join(cmd)}\\n\")\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True, encoding=\"utf-8\", errors=\"replace\")\n",
    "            print(result.stdout)\n",
    "            if result.returncode != 0:\n",
    "                print(f\"❌ Step '{name}' failed. Check the output above.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"✅ Step '{name}' completed.\\n\")\n",
    "\n",
    "        # Reload dataset after step execution if it exists\n",
    "        if os.path.exists(dataset_path):\n",
    "            return pd.read_csv(dataset_path)\n",
    "        else:\n",
    "            print(\"⚠️ Dataset not found after pipeline steps.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def calculate_presence_stats(df):\n",
    "    \"\"\"Calculate asset presence counts and percentages across Inventory and IPAM.\"\"\"\n",
    "    total_assets = len(df)\n",
    "    present_inventory = (df[\"missing_in_inventory\"] == 0).sum()\n",
    "    present_ipam = (df[\"missing_in_ipam\"] == 0).sum()\n",
    "    present_all = ((df[\"missing_in_inventory\"] == 0) & (df[\"missing_in_ipam\"] == 0)).sum()\n",
    "\n",
    "    return {\n",
    "        \"total_assets\": total_assets,\n",
    "        \"present_inventory\": present_inventory,\n",
    "        \"pct_inventory\": present_inventory / total_assets * 100,\n",
    "        \"present_ipam\": present_ipam,\n",
    "        \"pct_ipam\": present_ipam / total_assets * 100,\n",
    "        \"present_all\": present_all,\n",
    "        \"pct_all\": present_all / total_assets * 100\n",
    "    }\n",
    "\n",
    "\n",
    "def display_presence_summary(stats, scenario_name):\n",
    "    \"\"\"Display a quick summary of asset presence counts and percentages.\"\"\"\n",
    "    print(f\"\\n=== {scenario_name} Presence Summary ===\")\n",
    "    print(f\"Total Observability Assets: {stats['total_assets']:,}\")\n",
    "    print(f\"Present in Inventory: {stats['present_inventory']:,} ({stats['pct_inventory']:.1f}%)\")\n",
    "    print(f\"Present in IPAM: {stats['present_ipam']:,} ({stats['pct_ipam']:.1f}%)\")\n",
    "    print(f\"Present in BOTH: {stats['present_all']:,} ({stats['pct_all']:.1f}%)\")\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"Metric\": [\"Present in Inventory\", \"Present in IPAM\", \"Present in BOTH\"],\n",
    "        \"Count\": [stats[\"present_inventory\"], stats[\"present_ipam\"], stats[\"present_all\"]],\n",
    "        \"Percent\": [stats[\"pct_inventory\"], stats[\"pct_ipam\"], stats[\"pct_all\"]]\n",
    "    })\n",
    "    display(summary)\n",
    "\n",
    "\n",
    "def run_completeness_test(present_both, total_assets, threshold=0.75, alpha=0.05):\n",
    "    stat, p_value = sm.stats.proportions_ztest(\n",
    "        count=present_both,\n",
    "        nobs=total_assets,\n",
    "        value=threshold,\n",
    "        alternative=\"larger\"\n",
    "    )\n",
    "    return stat, p_value, p_value <= alpha\n",
    "\n",
    "    \n",
    "def evaluate_completeness(stats, scenario_name):\n",
    "    \"\"\"Run and display the completeness statistical test in a clean format.\"\"\"\n",
    "    z_stat, p_value, significant = run_completeness_test(stats[\"present_all\"], stats[\"total_assets\"])\n",
    "    result_text = \"✅ Reject H₀ – completeness is statistically significant.\" if significant else \"❌ Fail to reject H₀ – completeness is not statistically significant.\"\n",
    "\n",
    "    display(Markdown(f\"\"\"\n",
    "### Completeness Statistical Test ({scenario_name})\n",
    "\n",
    "| Metric               | Value                |\n",
    "|----------------------|----------------------|\n",
    "| Null Hypothesis (H₀) | Completeness ≤ 75%   |\n",
    "| Alternative (H₁)     | Completeness > 75%   |\n",
    "| Observed Rate        | {stats['pct_all']:.2f}% |\n",
    "| Z-statistic          | {z_stat:.4f}        |\n",
    "| P-value              | {p_value:.4e}       |\n",
    "| Alpha (α)            | 0.05               |\n",
    "| Result               | {result_text}       |\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# define a function to evaluate model performance\n",
    "def evaluate_model(report_path, model_name, benchmark_f1=0.80):\n",
    "    \"\"\"Evaluate model performance against the success benchmark.\"\"\"\n",
    "    if not os.path.exists(report_path):\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"f1_score\": None,\n",
    "            \"accuracy\": None,\n",
    "            \"result\": f\"⚠️ No report found for {model_name}\"\n",
    "        }\n",
    "    \n",
    "    with open(report_path, \"r\") as f:\n",
    "        report = json.load(f)\n",
    "    \n",
    "    f1_score = report[\"weighted avg\"][\"f1-score\"]\n",
    "    accuracy = report[\"accuracy\"]\n",
    "    result = \"✅ Meets benchmark\" if f1_score >= benchmark_f1 else \"❌ Below benchmark\"\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"result\": result\n",
    "    }\n",
    "    \n",
    "\n",
    "def format_metric(value, percent=False):\n",
    "    \"\"\"Safely format metrics for display.\"\"\"\n",
    "    if value is None:\n",
    "        return \"N/A\"\n",
    "    return f\"{value:.2%}\" if percent else f\"{value:.2f}\"\n",
    "\n",
    "\n",
    "def evaluate_models(inventory_report, ipam_report, scenario_name=None):\n",
    "    \"\"\"Evaluate and display predictive model performance in a clean table format.\"\"\"\n",
    "    inventory_eval = evaluate_model(inventory_report, \"Inventory Model\")\n",
    "    ipam_eval = evaluate_model(ipam_report, \"IPAM Model\")\n",
    "\n",
    "    scenario_title = f\" ({scenario_name})\" if scenario_name else \"\"\n",
    "\n",
    "    display(Markdown(f\"\"\"\n",
    "### Predictive Model Evaluation{scenario_title}\n",
    "\n",
    "| Model            | Accuracy | Weighted F1-Score | Result            |\n",
    "|------------------|----------|-------------------|-------------------|\n",
    "| {inventory_eval['model']} | {format_metric(inventory_eval['accuracy'], percent=True)} | {format_metric(inventory_eval['f1_score'])} | {inventory_eval['result']} |\n",
    "| {ipam_eval['model']}      | {format_metric(ipam_eval['accuracy'], percent=True)} | {format_metric(ipam_eval['f1_score'])} | {ipam_eval['result']} |\n",
    "\n",
    "**Benchmark:** F1-Score ≥ 0.80 required for success.\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "def display_report_images(scenario_name):\n",
    "    \"\"\"Display generated report images from the pipeline.\"\"\"\n",
    "    reports_dir = \"reports\"\n",
    "    image_extensions = ('.png', '.jpg', '.jpeg', '.gif')\n",
    "    found_images = []\n",
    "\n",
    "    for root, _, files in os.walk(reports_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(image_extensions):\n",
    "                found_images.append(os.path.join(root, file))\n",
    "\n",
    "    if found_images:\n",
    "        display(Markdown(f\"### Generated Report Images ({scenario_name})\"))\n",
    "        for img_path in found_images:\n",
    "            rel_path = os.path.relpath(img_path, reports_dir)\n",
    "            display(Markdown(f\"**{rel_path}**\"))\n",
    "            display(Image(filename=img_path))\n",
    "    else:\n",
    "        print(f\"No report images found in '{reports_dir}' for {scenario_name}.\")\n",
    "\n",
    "\n",
    "def display_conclusion(stats, scenario_name, completeness_significant, inventory_eval, ipam_eval, key_features=None):\n",
    "    \"\"\"Generate and display a comprehensive conclusion summary for a scenario.\"\"\"\n",
    "    meets_threshold = stats['pct_all'] >= 75\n",
    "    threshold_text = \"meets\" if meets_threshold else \"does not meet\"\n",
    "    significance_text = (\n",
    "        \"is statistically significant\" if completeness_significant else \"is not statistically significant\"\n",
    "    )\n",
    "\n",
    "    inv_model_text = \"met\" if inventory_eval[\"f1_score\"] and inventory_eval[\"f1_score\"] >= 0.80 else \"did not meet\"\n",
    "    ipam_model_text = \"met\" if ipam_eval[\"f1_score\"] and ipam_eval[\"f1_score\"] >= 0.80 else \"did not meet\"\n",
    "\n",
    "    conclusion_md = f\"\"\"\n",
    "## Conclusion ({scenario_name})\n",
    "\n",
    "The analysis confirms that the original asset presence rate across systems **{threshold_text}** the 75% threshold and {significance_text} based on the statistical test.\n",
    "\n",
    "Out of **{stats['total_assets']:,}** observability assets:\n",
    "- **{stats['present_inventory']:,}** ({stats['pct_inventory']:.1f}%) were found in the Inventory system.\n",
    "- **{stats['present_ipam']:,}** ({stats['pct_ipam']:.1f}%) were found in IPAM.\n",
    "- **{stats['present_all']:,}** ({stats['pct_all']:.1f}%) were present in both systems.\n",
    "\n",
    "Model performance evaluation:\n",
    "- The Inventory model **{inv_model_text}** the benchmark (F1 ≥ 0.80).\n",
    "- The IPAM model **{ipam_model_text}** the benchmark (F1 ≥ 0.80).\n",
    "\n",
    "\"\"\"\n",
    "    if key_features:\n",
    "        conclusion_md += f\"Key features driving missingness include: **{', '.join(key_features)}**.\\n\\n\"\n",
    "\n",
    "    conclusion_md += \"\"\"Beyond validating data completeness, these insights enable the organization to focus audit efforts \n",
    "on specific processes, automation, or staff actions most likely to cause data gaps. \n",
    "This data-driven approach not only supports immediate project goals but also lays the groundwork for \n",
    "long-term improvements in asset data quality and system reliability.\n",
    "\"\"\"\n",
    "    display(Markdown(conclusion_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the pipeline (cleans data, mlruns, models, reports)\n",
    "reset_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440da52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for analysis, if it is not present, trigger a mlflow run using the default pipeline settings\n",
    "df = run_pipeline(\"data/processed/labeled_asset_dataset_enriched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4968bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default scenario stats\n",
    "default_stats = calculate_presence_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_presence_summary(default_stats, \"Default Scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4262c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_completeness(default_stats, \"Default Scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3984af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save significance result for use in conclusion\n",
    "_, _, default_completeness_significant = run_completeness_test(\n",
    "    default_stats[\"present_all\"], default_stats[\"total_assets\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "inventory_eval = evaluate_model(\"reports/inventory/inventory_classification_report.json\", \"Inventory Model\")\n",
    "ipam_eval = evaluate_model(\"reports/ipam/ipam_classification_report.json\", \"IPAM Model\")\n",
    "\n",
    "evaluate_models(\"reports/inventory/inventory_classification_report.json\",\n",
    "                \"reports/ipam/ipam_classification_report.json\",\n",
    "                \"Default Scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e85483",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_report_images(\"Default Scenario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72061c03",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3659854",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_conclusion(\n",
    "    default_stats,\n",
    "    \"Default Scenario\",\n",
    "    completeness_significant=default_completeness_significant,\n",
    "    inventory_eval=inventory_eval,\n",
    "    ipam_eval=ipam_eval,\n",
    "    key_features=[\"region\", \"vendor\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00699d99",
   "metadata": {},
   "source": [
    "# Analysis demonstrating alternative data generation scenarios\n",
    "\n",
    "To validate that our ML model would make different predictions given different failures, we ran an alternative config to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_config(alt_config)\n",
    "display_config(alt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dde9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline for the alternative scenario\n",
    "alt_df = run_pipeline(\n",
    "    dataset_path=\"data/processed/labeled_asset_dataset_enriched.csv\",\n",
    "    alt_config=\"config/alt_scenario_generation_params.json\",\n",
    "    steps=[\"generate\", \"prepare\", \"train-both\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate presence statistics\n",
    "alt_stats = calculate_presence_stats(alt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display quick summary\n",
    "display_presence_summary(alt_stats, \"Alternative Scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524cf8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate completeness (statistical test)\n",
    "evaluate_completeness(alt_stats, \"Alternative Scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1311c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save significance result for use in conclusion\n",
    "_, _, alt_completeness_significant = run_completeness_test(\n",
    "    alt_stats[\"present_all\"],\n",
    "    alt_stats[\"total_assets\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7309fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "inventory_eval_alt = evaluate_model(\"reports/inventory/inventory_classification_report.json\", \"Inventory Model\")\n",
    "ipam_eval_alt = evaluate_model(\"reports/ipam/ipam_classification_report.json\", \"IPAM Model\")\n",
    "\n",
    "\n",
    "evaluate_models(\n",
    "    \"reports/inventory/inventory_classification_report.json\",\n",
    "    \"reports/ipam/ipam_classification_report.json\",\n",
    "    \"Alternative Scenario\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated report images\n",
    "display_report_images(\"Alternative Scenario\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ac4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display conclusion for alternative scenario\n",
    "display_conclusion(\n",
    "    alt_stats,\n",
    "    \"Alternative Scenario\",\n",
    "    completeness_significant=alt_completeness_significant,\n",
    "    inventory_eval=inventory_eval_alt,\n",
    "    ipam_eval=ipam_eval_alt,\n",
    "    key_features=[\"region\", \"vendor\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4f625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
